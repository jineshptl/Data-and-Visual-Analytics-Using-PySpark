{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Q3 [35 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notices\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: <strong>REMOVE</strong> any print statements added to cells with \"#export\" that are used for debugging purposes befrore submitting because they will crash the autograder in Gradescope. Any additional cells can be used for testing purposes at the bottom. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove any comment that says \"#export\" because that will crash the autograder in Gradescope. We use this comment to export your code in these cells for grading.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> import any additional libraries into this workbook.\n",
    "</div>\n",
    "\n",
    "All instructions, code comments, etc. in this notebook **are part of the assignment instructions**. That is, if there is instructions about completing a task in this notebook, that task is not optional.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    You <strong>must</strong> implement the following functions in this notebook to receive credit.\n",
    "</div>\n",
    "\n",
    "`user()` - 1 point\n",
    "\n",
    "`trip_statistics()` - 3 points\n",
    "\n",
    "`busiest_hour()` - 5 points\n",
    "\n",
    "`most_freq_pickup_locations()` - 5 points\n",
    "\n",
    "`avg_trip_distance_and_duration()` - 6 points\n",
    "\n",
    "`most_freq_peak_hour_fares()` - 10 points\n",
    "\n",
    "Each function will be auto-graded using different sets of parameters or data, to ensure that values are not hard-coded.  You may assume we will only use your code to work with data from the NYC-TLC dataset during auto-grading.\n",
    "\n",
    "In addition, you will also submit the resulting output csv from most_freq_peak_hour_fares() as output_large.csv.\n",
    "\n",
    "`output_large.csv` - 5 points\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remove or modify the following utility functions:\n",
    "</div>\n",
    "\n",
    "`load_data()`\n",
    "\n",
    "`main()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> remodify the below cell. It contains the function for loading data and all imports, and the function for running your code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#export\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE ANYTHING IN THIS CELL ####\n",
    "\n",
    "def load_data(size='small'):\n",
    "    # Loads the data for this question. Do not change this function.\n",
    "    # This function should only be called with the parameter 'small' or 'large'\n",
    "    \n",
    "    if size != 'small' and size != 'large':\n",
    "        print(\"Invalid size parameter provided. Use only 'small' or 'large'.\")\n",
    "        return\n",
    "    \n",
    "    input_bucket = \"s3://cse6242-hw3-q3\"\n",
    "    \n",
    "    # Load Trip Data\n",
    "    trips_path = '/'+size+'/yellow_tripdata*'\n",
    "    trips = spark.read.csv(input_bucket + trips_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Load Zone Data\n",
    "    zones_path = '/'+size+'/taxi*'\n",
    "    zones = spark.read.csv(input_bucket + zones_path, header=True, inferSchema=True)\n",
    "    \n",
    "    return trips, zones\n",
    "    \n",
    "def main(size, bucket):\n",
    "    # Runs your functions\n",
    "    trips, zones = load_data(size=size)\n",
    "    \n",
    "    print(\"User:\", user())\n",
    "    print()\n",
    "    \n",
    "    print(\"Trip Statistics:\")\n",
    "    ts = trip_statistics(trips)\n",
    "    ts.show()\n",
    "    print()\n",
    "    \n",
    "    print(\"Busiest Hour:\")\n",
    "    bh = busiest_hour(trips)\n",
    "    bh.show(24)\n",
    "    print()\n",
    "    \n",
    "    print(\"Most Frequent Pickup Locations:\")\n",
    "    mfpl = most_freq_pickup_locations(trips)\n",
    "    mfpl.show()\n",
    "    print()\n",
    "    \n",
    "    print(\"Average Trip Distance and Duration:\")\n",
    "    atdd = avg_trip_distance_and_duration(trips)\n",
    "    atdd.show(n=24)\n",
    "    print()\n",
    "    \n",
    "    print(\"Most Frequent Peak Hour Fares:\")\n",
    "    mfphf = most_freq_peak_hour_fares(trips, zones)\n",
    "    mfphf.show()\n",
    "    mfphf.coalesce(1).write.option(\"header\",\"true\").mode(\"overwrite\").csv('{}/output_{}'.format(bucket, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the below functions for this assignment:\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    WARNING: Do <strong>NOT</strong> change any function inputs or outputs, and ensure that the dataframes your code returns align with the schema definitions commented in each function. Do <strong>NOT</strong> remove the #export comment from each of the code blocks either. This can prevent your code from being converted to a python file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 [1 pt] Update the `user()` function\n",
    "This function should return your GT username, eg: gburdell3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def user():\n",
    "    return 'jpate685'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 [3 pts] Update the `trip_statistics()` function\n",
    "This function performs exploratory data analysis on the column trip_distance. Compute basic statistics (count, mean, stdev, min, max) for trip_distance. \n",
    "\n",
    "Example output formatting:\n",
    "\n",
    "```\n",
    "+-------+------------------+\n",
    "|summary|     trip_distance|\n",
    "+-------+------------------+\n",
    "|  count|           xxxxxxx|\n",
    "|   mean|           xxxxxxx|\n",
    "| stddev|           xxxxxxx|\n",
    "|    min|           xxxxxxx|\n",
    "|    max|           xxxxxxx|\n",
    "+-------+------------------+\n",
    "```\n",
    "Tip: Is there a PySpark Dataframe function you can use to solve this in a single line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trip_statistics(trips):\n",
    "    from pyspark.sql import functions as F\n",
    "    return trips.select(\"trip_distance\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 [5 pts] Update the `busiest_hour()` function\n",
    "\n",
    "Determine the hour of the day with the highest number of trips. Display the hour (0-23) and the corresponding trip count. \n",
    "\n",
    "Returns a PySpark DataFrame with a single row showing the hour with the highest trip count and the corresponding number of trips. Schema (hour, trip_count) \n",
    "\n",
    "Example output formatting:\n",
    "\n",
    "```\n",
    "+----+----------+\n",
    "|hour|trip_count|\n",
    "+----+----------+\n",
    "|  xx|    xxxxxx|\n",
    "+----+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def busiest_hour(trips):\n",
    "    from pyspark.sql.functions import hour, col, count\n",
    "\n",
    "    if trips is None:\n",
    "        return None\n",
    "\n",
    "    df = trips.withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "    df = df.groupBy(\"hour\").agg(count(\"*\").alias(\"trip_count\"))\n",
    "    df = df.orderBy(col(\"trip_count\").desc()).limit(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 [5 pts] Update the `most_freq_pickup_locations()` function\n",
    "Top 10 Most Frequent Pickup Locations\n",
    "\n",
    "Identify the top 10 pickup locations (by PULocationID) with the highest number of trips. Display the location IDs along with their corresponding trip counts.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+------------+----------+\n",
    "|PULocationID|trip_count|\n",
    "+------------+----------+\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         xxx|    xxxxxx|\n",
    "|         ...|    ......|\n",
    "+------------+----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_freq_pickup_locations(trips): \n",
    "    from pyspark.sql.functions import col, count\n",
    "\n",
    "    df = trips.groupBy(\"PULocationID\").agg(count(\"*\").alias(\"trip_count\"))\n",
    "    df = df.orderBy(col(\"trip_count\").desc()).limit(10)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 [6 pts] Update the `avg_trip_distance_and_duration()` function\n",
    "Average Trip Distance and Duration by Hour\n",
    "\n",
    "Calculate the average trip distance and average trip duration in minutes (i.e., divided by 60) for each hour of the day (0-23). Display the hour along with the corresponding averages. To be a valid trip, it must have a non-null timestamp and a trip distance greater than zero.\n",
    "\n",
    "Note: You can use `unix_timestamp` to help with calculating the duration. If there are null or invalid timestamps, you will want to handle those accordingly. \n",
    "\n",
    "Expected Output:\n",
    "\n",
    "A table with 24 rows showing each hour (0-23) along with the average trip distance and average trip duration for that hour.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+----+------------------+------------------+\n",
    "|hour| avg_trip_distance| avg_trip_duration|\n",
    "+----+------------------+------------------+\n",
    "|   0|           xxxxxxx|           xxxxxxx|\n",
    "|   1|           xxxxxxx|           xxxxxxx|\n",
    "|   2|           xxxxxxx|           xxxxxxx|\n",
    "|   3|           xxxxxxx|           xxxxxxx|\n",
    "| ...|               ...|               ...|\n",
    "|  23|           xxxxxxx|           xxxxxxx|\n",
    "+----+------------------+------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def avg_trip_distance_and_duration(trips):\n",
    "    from pyspark.sql.functions import col, hour, unix_timestamp, avg\n",
    "\n",
    "    # Filter valid trips\n",
    "    df = trips.filter((col(\"trip_distance\") > 0) &\n",
    "                      col(\"tpep_pickup_datetime\").isNotNull() &\n",
    "                      col(\"tpep_dropoff_datetime\").isNotNull())\n",
    "\n",
    "    # Extract hour and compute trip duration in minutes\n",
    "    df = df.withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "    df = df.withColumn(\"trip_duration\", \n",
    "                       (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60)\n",
    "\n",
    "    # Group by hour and compute averages\n",
    "    df = df.groupBy(\"hour\").agg(\n",
    "        avg(\"trip_distance\").alias(\"avg_trip_distance\"),\n",
    "        avg(\"trip_duration\").alias(\"avg_trip_duration\")\n",
    "    ).orderBy(\"hour\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 [10 pts] Update the `most_freq_peak_hour_fares()` function\n",
    "Top 10 Most Frequent Routes During Peak Hour \n",
    "\n",
    "Identify the top 10 most frequent routes (combinations of PULocationID and DOLocationID) during peak hours (7 AM - 9 AM and 4 PM - 7 PM). Peak hours can be defined as 7 <= hour < 9 and 16 <= hour < 19. Display the pickup and drop-off location ID pairs along with their trip counts and average total fare rounded to two decimal places. \n",
    "\n",
    "Note: A route must have a different drop off location from pickup location to be considered a valid route.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "A table showing the top 10 routes during peak hours with their trip counts and average total fare rounded to two decimal places. Each route is represented as a combination of PULocationID-DOLocationID and PULocationID should not be the same as DOLocationID.\n",
    "\n",
    "Example output formatting:\n",
    "```\n",
    "+------------+------+------------+------+----------+--------------+\n",
    "|PULocationID|PUZone|DOLocationID|DOZone|trip_count|avg_total_fare|\n",
    "+------------+------+------------+------+----------+--------------+\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|xxx         |xxx   |xxx         |xxx   |xxx       |xx.xx         |\n",
    "|...         |...   |...         |...   |...       |...           |\n",
    "+------------+------+------------+------+----------+--------------|\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def most_freq_peak_hour_fares(trips, zones):\n",
    "    from pyspark.sql.functions import col, hour, count, avg, round as spark_round\n",
    "\n",
    "    # Define peak hours condition\n",
    "    trips = trips.withColumn(\"hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "    peak_trips = trips.filter(\n",
    "        ((col(\"hour\") >= 7) & (col(\"hour\") < 9)) | ((col(\"hour\") >= 16) & (col(\"hour\") < 19))\n",
    "    )\n",
    "\n",
    "    # Filter out trips with same pickup and dropoff\n",
    "    peak_trips = peak_trips.filter(col(\"PULocationID\") != col(\"DOLocationID\"))\n",
    "\n",
    "    # Group by PULocationID and DOLocationID, compute trip count and average total fare\n",
    "    route_stats = peak_trips.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "        count(\"*\").alias(\"trip_count\"),\n",
    "        spark_round(avg(\"total_amount\"), 2).alias(\"avg_total_fare\")\n",
    "    )\n",
    "\n",
    "    # Join with zone names for pickup and dropoff\n",
    "    route_stats = route_stats.join(zones.withColumnRenamed(\"LocationID\", \"PULocationID\")\n",
    "                                   .withColumnRenamed(\"Zone\", \"PUZone\"), on=\"PULocationID\", how=\"left\")\n",
    "    route_stats = route_stats.join(zones.withColumnRenamed(\"LocationID\", \"DOLocationID\")\n",
    "                                   .withColumnRenamed(\"Zone\", \"DOZone\"), on=\"DOLocationID\", how=\"left\")\n",
    "\n",
    "    # Select final columns and top 10 routes\n",
    "    df = route_stats.select(\"PULocationID\", \"PUZone\", \"DOLocationID\", \"DOZone\", \"trip_count\", \"avg_total_fare\")\n",
    "    df = df.orderBy(col(\"trip_count\").desc()).limit(10)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Once you have finished coding, you can export the notebook from `Notebook Explorer` by selecting your notebook and clicking `Export File` from the Actions dropdown.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    You may use the below cell for any additional testing you need to do, however any code implemented below will not be ran or used when grading. You can run the main function over the different sized datasets for testing your functions or you run them individually like in the examples below. To get the final output csv, you will need to run most_freq_peak_hour_fares(trips, zones) using the large dataset and write the resulting dataframe to a csv. The main function will do this for you, or you can do it yourself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trips, zones = \u001b[43mload_data\u001b[49m(\u001b[33m'\u001b[39m\u001b[33msmall\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "trips, zones = load_data('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trips' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ts = trip_statistics(\u001b[43mtrips\u001b[49m)\n\u001b[32m      2\u001b[39m ts.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'trips' is not defined"
     ]
    }
   ],
   "source": [
    "ts = trip_statistics(trips)\n",
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mlarge\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33ms3://cse6242-jpatel685\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "main('large', 's3://cse6242-jpatel685')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
